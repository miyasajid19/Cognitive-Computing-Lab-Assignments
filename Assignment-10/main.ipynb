{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M5FlJoDjS4ZQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5FlJoDjS4ZQ",
        "outputId": "11e7549d-4662-4623-92f0-f30b70682c1a"
      },
      "outputs": [],
      "source": [
        "# prompt: uninstal nltk and install version 3.8.0\n",
        "!pip uninstall nltk\n",
        "!pip install nltk==3.8.0\n",
        "!pip install textblob\n",
        "!pip install wordcloud\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "510b3ede",
      "metadata": {
        "id": "510b3ede"
      },
      "source": [
        "# NLP using Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e9252e8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9252e8",
        "outputId": "d80dfdcc-0800-4744-94b7-6d899147c4b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\miyas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\miyas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\miyas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\miyas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2470c29",
      "metadata": {
        "id": "b2470c29"
      },
      "source": [
        "\n",
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32dca58c",
      "metadata": {
        "id": "32dca58c"
      },
      "source": [
        "1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6e11a445",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "6e11a445",
        "outputId": "ba626113-43b2-4174-899f-b698f6a30a09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nObito Uchiha, a character from the Naruto series, is one of the most complex and tragic figures in the story.\\nOnce a cheerful and optimistic ninja, Obito's life took a dark turn after a mission went horribly wrong, leaving him presumed dead.\\nRescued by Madara Uchiha, he was manipulated into adopting a nihilistic worldview and became a key figure in the Akatsuki organization.\\nObito's ultimate goal was to create an ideal world through the Infinite Tsukuyomi, a plan that involved casting the entire world into a genjutsu.\\nDespite his descent into darkness, Obito's character arc is one of redemption, as he ultimately sacrifices himself to protect his friends and atone for his past actions.\\nHis story serves as a powerful exploration of loss, identity, and the possibility of redemption even in the face of overwhelming despair.\\n\""
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paragraph='''\n",
        "Obito Uchiha, a character from the Naruto series, is one of the most complex and tragic figures in the story.\n",
        "Once a cheerful and optimistic ninja, Obito's life took a dark turn after a mission went horribly wrong, leaving him presumed dead.\n",
        "Rescued by Madara Uchiha, he was manipulated into adopting a nihilistic worldview and became a key figure in the Akatsuki organization.\n",
        "Obito's ultimate goal was to create an ideal world through the Infinite Tsukuyomi, a plan that involved casting the entire world into a genjutsu.\n",
        "Despite his descent into darkness, Obito's character arc is one of redemption, as he ultimately sacrifices himself to protect his friends and atone for his past actions.\n",
        "His story serves as a powerful exploration of loss, identity, and the possibility of redemption even in the face of overwhelming despair.\n",
        "'''\n",
        "paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a14c46cb",
      "metadata": {
        "id": "a14c46cb"
      },
      "source": [
        "2. Convert text to lowercase and remove punctuation.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d0ef7e93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "d0ef7e93",
        "outputId": "910a3461-74d7-4a43-94a7-ef27202dd420"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nobito uchiha a character from the naruto series is one of the most complex and tragic figures in the story\\nonce a cheerful and optimistic ninja obitos life took a dark turn after a mission went horribly wrong leaving him presumed dead\\nrescued by madara uchiha he was manipulated into adopting a nihilistic worldview and became a key figure in the akatsuki organization\\nobitos ultimate goal was to create an ideal world through the infinite tsukuyomi a plan that involved casting the entire world into a genjutsu\\ndespite his descent into darkness obitos character arc is one of redemption as he ultimately sacrifices himself to protect his friends and atone for his past actions\\nhis story serves as a powerful exploration of loss identity and the possibility of redemption even in the face of overwhelming despair\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "paragraph=paragraph.lower()\n",
        "paragraph=paragraph.translate(str.maketrans('', '', string.punctuation))\n",
        "paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6834af5c",
      "metadata": {
        "id": "6834af5c"
      },
      "source": [
        "3. Tokenize the text into words and sentences.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c4c2695e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4c2695e",
        "outputId": "160d382b-3cd8-4c27-bfb0-6b2c4106d129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(136, 1)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words=word_tokenize(paragraph)\n",
        "sentences=sent_tokenize(paragraph)\n",
        "len(words), len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fedd060",
      "metadata": {
        "id": "4fedd060"
      },
      "source": [
        "4. Remove stopwords (using NLTK's stopwords list).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a27d4691",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a27d4691",
        "outputId": "f479c595-6077-4ac2-fe1e-0f8c0fada2d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_words=set(stopwords.words('english'))\n",
        "filtered_words=[word for word in words if word not in stop_words]\n",
        "len(filtered_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707b5e6b",
      "metadata": {
        "id": "707b5e6b"
      },
      "source": [
        "5. Display word frequency distribution (excluding stopwords).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce303303",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "ce303303",
        "outputId": "3f8650ce-caca-4476-ac34-e7590a4257f6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>obitos</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>uchiha</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>redemption</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>world</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>character</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>story</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>one</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>atone</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>became</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>akatsuki</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         word  count\n",
              "0      obitos      3\n",
              "1      uchiha      2\n",
              "2  redemption      2\n",
              "3       world      2\n",
              "4   character      2\n",
              "5       story      2\n",
              "6         one      2\n",
              "7       atone      1\n",
              "8      became      1\n",
              "9    akatsuki      1"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(filtered_words, columns=['word'])\n",
        "df.value_counts().reset_index(name='count').rename(columns={'index':'word'}).sort_values(by='count', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e0aac6a",
      "metadata": {
        "id": "4e0aac6a"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "574772b2",
      "metadata": {
        "id": "574772b2"
      },
      "source": [
        "1. Take the tokenized words from Ques on 1 (after stopword removal).\n",
        "2. remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "28637cc5",
      "metadata": {
        "id": "28637cc5"
      },
      "outputs": [],
      "source": [
        "filtered_words=list(set(filtered_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7922f3ef",
      "metadata": {
        "id": "7922f3ef"
      },
      "source": [
        "3. Perform stemming with PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "37fb3aae",
      "metadata": {
        "id": "37fb3aae"
      },
      "outputs": [],
      "source": [
        "stemmer=PorterStemmer()\n",
        "stemmed_words=[stemmer.stem(word) for word in filtered_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "82118bc2",
      "metadata": {
        "id": "82118bc2"
      },
      "outputs": [],
      "source": [
        "lancaster_stemmer=LancasterStemmer()\n",
        "lancaster_words=[lancaster_stemmer.stem(word) for word in filtered_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1374f83",
      "metadata": {
        "id": "b1374f83"
      },
      "source": [
        "4. Apply lemmatization using NLTK's  WordNetLemmatizer .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "52ab08a5",
      "metadata": {
        "id": "52ab08a5"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer=WordNetLemmatizer()\n",
        "wordnet_words=[wordnet_lemmatizer.lemmatize(word) for word in filtered_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddf9d097",
      "metadata": {
        "id": "ddf9d097"
      },
      "source": [
        "5. Compare the stemmed and lemma zed outputs and explain when you’d prefer one over\n",
        "the other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a29ab32b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "a29ab32b",
        "outputId": "abb207f9-2ad4-43dc-9170-62fa921790fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>stemmed</th>\n",
              "      <th>lancaster</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>exploration</td>\n",
              "      <td>explor</td>\n",
              "      <td>expl</td>\n",
              "      <td>exploration</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rescued</td>\n",
              "      <td>rescu</td>\n",
              "      <td>rescu</td>\n",
              "      <td>rescued</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>became</td>\n",
              "      <td>becam</td>\n",
              "      <td>becam</td>\n",
              "      <td>became</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>world</td>\n",
              "      <td>world</td>\n",
              "      <td>world</td>\n",
              "      <td>world</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>turn</td>\n",
              "      <td>turn</td>\n",
              "      <td>turn</td>\n",
              "      <td>turn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>character</td>\n",
              "      <td>charact</td>\n",
              "      <td>charact</td>\n",
              "      <td>character</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>madara</td>\n",
              "      <td>madara</td>\n",
              "      <td>madar</td>\n",
              "      <td>madara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>possibility</td>\n",
              "      <td>possibl</td>\n",
              "      <td>poss</td>\n",
              "      <td>possibility</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>story</td>\n",
              "      <td>stori</td>\n",
              "      <td>story</td>\n",
              "      <td>story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>leaving</td>\n",
              "      <td>leav</td>\n",
              "      <td>leav</td>\n",
              "      <td>leaving</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>70 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       original  stemmed lancaster   lemmatized\n",
              "0   exploration   explor      expl  exploration\n",
              "1       rescued    rescu     rescu      rescued\n",
              "2        became    becam     becam       became\n",
              "3         world    world     world        world\n",
              "4          turn     turn      turn         turn\n",
              "..          ...      ...       ...          ...\n",
              "65    character  charact   charact    character\n",
              "66       madara   madara     madar       madara\n",
              "67  possibility  possibl      poss  possibility\n",
              "68        story    stori     story        story\n",
              "69      leaving     leav      leav      leaving\n",
              "\n",
              "[70 rows x 4 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df=pd.DataFrame({'original':filtered_words, 'stemmed':stemmed_words, 'lancaster':lancaster_words, 'lemmatized':wordnet_words})\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e5ad00",
      "metadata": {
        "id": "f7e5ad00"
      },
      "source": [
        "# Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fda826",
      "metadata": {
        "id": "c0fda826"
      },
      "source": [
        "1. Use CountVectorizer to generate the Bag of Words representa on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "46cc2cfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "46cc2cfa",
        "outputId": "25fad7cb-b550-4742-bedc-56f26506a9e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actions</th>\n",
              "      <th>adopting</th>\n",
              "      <th>akatsuki</th>\n",
              "      <th>arc</th>\n",
              "      <th>atone</th>\n",
              "      <th>became</th>\n",
              "      <th>casting</th>\n",
              "      <th>character</th>\n",
              "      <th>cheerful</th>\n",
              "      <th>complex</th>\n",
              "      <th>...</th>\n",
              "      <th>tragic</th>\n",
              "      <th>tsukuyomi</th>\n",
              "      <th>turn</th>\n",
              "      <th>uchiha</th>\n",
              "      <th>ultimate</th>\n",
              "      <th>ultimately</th>\n",
              "      <th>went</th>\n",
              "      <th>world</th>\n",
              "      <th>worldview</th>\n",
              "      <th>wrong</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   actions  adopting  akatsuki  arc  atone  became  casting  character  \\\n",
              "0        1         1         1    1      1       1        1          1   \n",
              "\n",
              "   cheerful  complex  ...  tragic  tsukuyomi  turn  uchiha  ultimate  \\\n",
              "0         1        1  ...       1          1     1       1         1   \n",
              "\n",
              "   ultimately  went  world  worldview  wrong  \n",
              "0           1     1      1          1      1  \n",
              "\n",
              "[1 rows x 70 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the filtered words into Bag of Words representation\n",
        "bow_matrix = vectorizer.fit_transform([' '.join(filtered_words)])\n",
        "\n",
        "# Convert the matrix to a DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "bow_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163548f7",
      "metadata": {
        "id": "163548f7"
      },
      "source": [
        "2. Use TfidfVectorizer to compute TF-IDF scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a479af6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "a479af6a",
        "outputId": "aa1a15e5-3ff4-4a08-9bb2-eb9b57d190aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actions</th>\n",
              "      <th>adopting</th>\n",
              "      <th>akatsuki</th>\n",
              "      <th>arc</th>\n",
              "      <th>atone</th>\n",
              "      <th>became</th>\n",
              "      <th>casting</th>\n",
              "      <th>character</th>\n",
              "      <th>cheerful</th>\n",
              "      <th>complex</th>\n",
              "      <th>...</th>\n",
              "      <th>tragic</th>\n",
              "      <th>tsukuyomi</th>\n",
              "      <th>turn</th>\n",
              "      <th>uchiha</th>\n",
              "      <th>ultimate</th>\n",
              "      <th>ultimately</th>\n",
              "      <th>went</th>\n",
              "      <th>world</th>\n",
              "      <th>worldview</th>\n",
              "      <th>wrong</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>...</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "      <td>0.119523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    actions  adopting  akatsuki       arc     atone    became   casting  \\\n",
              "0  0.119523  0.119523  0.119523  0.119523  0.119523  0.119523  0.119523   \n",
              "\n",
              "   character  cheerful   complex  ...    tragic  tsukuyomi      turn  \\\n",
              "0   0.119523  0.119523  0.119523  ...  0.119523   0.119523  0.119523   \n",
              "\n",
              "     uchiha  ultimate  ultimately      went     world  worldview     wrong  \n",
              "0  0.119523  0.119523    0.119523  0.119523  0.119523   0.119523  0.119523  \n",
              "\n",
              "[1 rows x 70 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer instance\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the filtered words into TF-IDF representation\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform([' '.join(filtered_words)])\n",
        "\n",
        "# Convert the matrix to a DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "tfidf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664a6e5a",
      "metadata": {
        "id": "664a6e5a"
      },
      "source": [
        "3. Print and interpret the top 3 keywords from each text using TF-IDF.```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "da90be49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "da90be49",
        "outputId": "8a1060c4-be3c-4f04-d549-d0de2a9242f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Keyword</th>\n",
              "      <th>TF-IDF Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>actions</td>\n",
              "      <td>0.119523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>adopting</td>\n",
              "      <td>0.119523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>akatsuki</td>\n",
              "      <td>0.119523</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Keyword  TF-IDF Score\n",
              "0   actions      0.119523\n",
              "1  adopting      0.119523\n",
              "2  akatsuki      0.119523"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract the top 3 keywords for each text based on TF-IDF scores\n",
        "top_keywords = tfidf_df.T.nlargest(3, 0).reset_index()\n",
        "top_keywords.columns = ['Keyword', 'TF-IDF Score']\n",
        "top_keywords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabde505",
      "metadata": {
        "id": "dabde505"
      },
      "source": [
        "# Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs Blockchain).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59bd7328",
      "metadata": {
        "id": "59bd7328"
      },
      "source": [
        "1. Preprocess and tokenize both texts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9ddb6f19",
      "metadata": {
        "id": "9ddb6f19"
      },
      "outputs": [],
      "source": [
        "# Text 1: Artificial Intelligence (AI)\n",
        "text_ai = \"\"\"\n",
        "Artificial Intelligence (AI) is a branch of computer science that focuses on creating systems capable of performing tasks that typically require human intelligence.\n",
        "These tasks include natural language processing, image recognition, decision-making, and problem-solving.\n",
        "AI is widely used in applications such as virtual assistants, autonomous vehicles, and predictive analytics.\n",
        "It leverages machine learning and deep learning techniques to improve its performance over time.\n",
        "\"\"\"\n",
        "\n",
        "# Text 2: Blockchain\n",
        "text_blockchain = \"\"\"\n",
        "Blockchain is a decentralized and distributed ledger technology that ensures secure and transparent record-keeping.\n",
        "It operates through a network of nodes that validate and store transactions in blocks, which are linked together in a chain.\n",
        "Blockchain is the backbone of cryptocurrencies like Bitcoin and Ethereum, but its applications extend to supply chain management, healthcare, and voting systems.\n",
        "Its immutability and transparency make it a revolutionary technology for trust-based systems.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796d7266",
      "metadata": {
        "id": "796d7266"
      },
      "source": [
        "2. Calculate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "32221bd0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32221bd0",
        "outputId": "80b83156-9089-4472-8db6-f8bef8688fa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['artificial',\n",
              "  'intelligence',\n",
              "  'ai',\n",
              "  'is',\n",
              "  'a',\n",
              "  'branch',\n",
              "  'of',\n",
              "  'computer',\n",
              "  'science',\n",
              "  'that',\n",
              "  'focuses',\n",
              "  'on',\n",
              "  'creating',\n",
              "  'systems',\n",
              "  'capable',\n",
              "  'of',\n",
              "  'performing',\n",
              "  'tasks',\n",
              "  'that',\n",
              "  'typically',\n",
              "  'require',\n",
              "  'human',\n",
              "  'intelligence',\n",
              "  'these',\n",
              "  'tasks',\n",
              "  'include',\n",
              "  'natural',\n",
              "  'language',\n",
              "  'processing',\n",
              "  'image',\n",
              "  'recognition',\n",
              "  'decisionmaking',\n",
              "  'and',\n",
              "  'problemsolving',\n",
              "  'ai',\n",
              "  'is',\n",
              "  'widely',\n",
              "  'used',\n",
              "  'in',\n",
              "  'applications',\n",
              "  'such',\n",
              "  'as',\n",
              "  'virtual',\n",
              "  'assistants',\n",
              "  'autonomous',\n",
              "  'vehicles',\n",
              "  'and',\n",
              "  'predictive',\n",
              "  'analytics',\n",
              "  'it',\n",
              "  'leverages',\n",
              "  'machine',\n",
              "  'learning',\n",
              "  'and',\n",
              "  'deep',\n",
              "  'learning',\n",
              "  'techniques',\n",
              "  'to',\n",
              "  'improve',\n",
              "  'its',\n",
              "  'performance',\n",
              "  'over',\n",
              "  'time'],\n",
              " ['blockchain',\n",
              "  'is',\n",
              "  'a',\n",
              "  'decentralized',\n",
              "  'and',\n",
              "  'distributed',\n",
              "  'ledger',\n",
              "  'technology',\n",
              "  'that',\n",
              "  'ensures',\n",
              "  'secure',\n",
              "  'and',\n",
              "  'transparent',\n",
              "  'recordkeeping',\n",
              "  'it',\n",
              "  'operates',\n",
              "  'through',\n",
              "  'a',\n",
              "  'network',\n",
              "  'of',\n",
              "  'nodes',\n",
              "  'that',\n",
              "  'validate',\n",
              "  'and',\n",
              "  'store',\n",
              "  'transactions',\n",
              "  'in',\n",
              "  'blocks',\n",
              "  'which',\n",
              "  'are',\n",
              "  'linked',\n",
              "  'together',\n",
              "  'in',\n",
              "  'a',\n",
              "  'chain',\n",
              "  'blockchain',\n",
              "  'is',\n",
              "  'the',\n",
              "  'backbone',\n",
              "  'of',\n",
              "  'cryptocurrencies',\n",
              "  'like',\n",
              "  'bitcoin',\n",
              "  'and',\n",
              "  'ethereum',\n",
              "  'but',\n",
              "  'its',\n",
              "  'applications',\n",
              "  'extend',\n",
              "  'to',\n",
              "  'supply',\n",
              "  'chain',\n",
              "  'management',\n",
              "  'healthcare',\n",
              "  'and',\n",
              "  'voting',\n",
              "  'systems',\n",
              "  'its',\n",
              "  'immutability',\n",
              "  'and',\n",
              "  'transparency',\n",
              "  'make',\n",
              "  'it',\n",
              "  'a',\n",
              "  'revolutionary',\n",
              "  'technology',\n",
              "  'for',\n",
              "  'trustbased',\n",
              "  'systems'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocessing and tokenizing Text 1 (AI)\n",
        "text_ai_cleaned = text_ai.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "text_ai_tokens = word_tokenize(text_ai_cleaned)\n",
        "\n",
        "# Preprocessing and tokenizing Text 2 (Blockchain)\n",
        "text_blockchain_cleaned = text_blockchain.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "text_blockchain_tokens = word_tokenize(text_blockchain_cleaned)\n",
        "\n",
        "text_ai_tokens, text_blockchain_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b73364b3",
      "metadata": {
        "id": "b73364b3"
      },
      "source": [
        "\n",
        "a. Jaccard Similarity using sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a76b43ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a76b43ac",
        "outputId": "cd00cb51-1a1d-4541-ee4d-c3ff7d22d537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Jaccard Similarity: 0.11702127659574468'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# a. Jaccard Similarity using sets\n",
        "set_ai = set(text_ai_tokens)\n",
        "set_blockchain = set(text_blockchain_tokens)\n",
        "jaccard_similarity = len(set_ai.intersection(set_blockchain)) / len(set_ai.union(set_blockchain))\n",
        "f\"Jaccard Similarity: {jaccard_similarity}\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c889cd",
      "metadata": {
        "id": "52c889cd"
      },
      "source": [
        "\n",
        "b. Cosine Similarity using TfidfVectorizer + cosine_similarity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "caaff620",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caaff620",
        "outputId": "2a8a8a54-cc76-4bf6-8e50-1fa2c41372c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.2691815582148356\n"
          ]
        }
      ],
      "source": [
        "# b. Cosine Similarity using TfidfVectorizer + cosine_similarity()\n",
        "tfidf_matrix_tech = tfidf_vectorizer.fit_transform([text_ai_cleaned, text_blockchain_cleaned])\n",
        "cosine_sim = cosine_similarity(tfidf_matrix_tech[0:1], tfidf_matrix_tech[1:2])\n",
        "print(f\"Cosine Similarity: {cosine_sim[0][0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d93dc381",
      "metadata": {
        "id": "d93dc381"
      },
      "source": [
        "c. Analyze which similarity metric gives be er insights in your case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b44b31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80b44b31",
        "outputId": "f54950db-2461-40ee-cde4-5287f3641391"
      },
      "outputs": [],
      "source": [
        "# c. Analysis\n",
        "print('''Jaccard Similarity focuses on the overlap of unique tokens, while Cosine Similarity considers the frequency and importance of terms.\n",
        "In this case, Cosine Similarity provides better insights as it captures the importance of terms using TF-IDF weights.''')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d16f2f97",
      "metadata": {
        "id": "d16f2f97"
      },
      "source": [
        "# Q5. Write a short review for a product or service.\n",
        "1. Use TextBlob or VADER to find polarity & subjec vity for each review.\n",
        "2. Classify reviews into Posi ve / Nega ve / Neutral.\n",
        "3. Create a word cloud using the wordcloud library for all posi ve reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "64f79bfe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "64f79bfe",
        "outputId": "16f7344b-5c14-4ea4-9e8b-c8f7f3a76f0c"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'wordcloud'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Step 1: Write a short review\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Write a short review\n",
        "reviews = [\n",
        "    \"The product is amazing! It exceeded my expectations and works perfectly.\",\n",
        "    \"The service was okay, but it could have been better.\",\n",
        "    \"I am very disappointed with the product. It broke after one use.\"\n",
        "]\n",
        "\n",
        "# Step 2: Analyze polarity and subjectivity\n",
        "review_analysis = []\n",
        "for review in reviews:\n",
        "    blob = TextBlob(review)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    sentiment = \"Positive\" if polarity > 0 else \"Negative\" if polarity < 0 else \"Neutral\"\n",
        "    review_analysis.append((review, polarity, subjectivity, sentiment))\n",
        "\n",
        "# Convert analysis to DataFrame for better visualization\n",
        "review_df = pd.DataFrame(review_analysis, columns=[\"Review\", \"Polarity\", \"Subjectivity\", \"Sentiment\"])\n",
        "print(review_df)\n",
        "\n",
        "# Step 3: Create a word cloud for positive reviews\n",
        "positive_reviews = \" \".join(review_df[review_df[\"Sentiment\"] == \"Positive\"][\"Review\"])\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(positive_reviews)\n",
        "\n",
        "# Display the word cloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Word Cloud for Positive Reviews\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e882554a",
      "metadata": {
        "id": "e882554a"
      },
      "source": [
        "# Q6. Choose your own paragraph (~100 words) as training data.\n",
        "1. Tokenize text using Tokenizer() from keras.preprocessing.text\n",
        "2. Create input sequences and build a simple LSTM or Dense model\n",
        "3. Train the model and generate 2–3 new lines of text star ng from any seed word you\n",
        "provide.```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aazQHOlnU5ev",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aazQHOlnU5ev",
        "outputId": "d45b4ecb-4480-4854-8ff9-001b209ffe00"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Choose a paragraph as training data\n",
        "training_text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming the world. It powers applications like virtual assistants, autonomous vehicles, and predictive analytics.\n",
        "AI leverages machine learning and deep learning to solve complex problems. The technology is evolving rapidly, enabling breakthroughs in healthcare, finance, and education.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([training_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences\n",
        "input_sequences = []\n",
        "for line in training_text.split(\"\\n\"):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Split data into predictors and label\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = np.array(y)\n",
        "\n",
        "# Step 2: Build a simple LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Step 3: Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Generate new text\n",
        "seed_text = \"AI is\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == np.argmax(predicted):\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
